"""
ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
"""
import asyncio
import sys
import json
from pathlib import Path

# backend/appë¥¼ Python pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent / "backend"))

from app.crawler.naver_crawler import NaverRealEstateCrawler


async def test_crawler():
    """í¬ë¡¤ëŸ¬ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""

    print("=" * 80)
    print("ğŸ§ª ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
    print("=" * 80)

    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” (headless=Falseë¡œ í•˜ë©´ ë¸Œë¼ìš°ì €ê°€ ë³´ì„)
    async with NaverRealEstateCrawler(headless=False) as crawler:

        # 1. ë‹¨ì§€ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print("\n" + "=" * 80)
        print("TEST 1: ë‹¨ì§€ ê²€ìƒ‰")
        print("=" * 80)

        complexes = await crawler.search_complexes("ë˜ë¯¸ì•ˆ", limit=5)

        if complexes:
            print(f"\nâœ… ê²€ìƒ‰ ê²°ê³¼ {len(complexes)}ê°œ:")
            for i, complex_info in enumerate(complexes, 1):
                print(f"\n{i}. {complex_info.get('name', 'Unknown')}")
                print(f"   ID: {complex_info.get('id')}")
                print(f"   ì£¼ì†Œ: {complex_info.get('address')}")
                print(f"   URL: {complex_info.get('url')}")

            # ì²« ë²ˆì§¸ ë‹¨ì§€ë¡œ ìƒì„¸ ì •ë³´ í…ŒìŠ¤íŠ¸
            first_complex = complexes[0]
            complex_id = first_complex.get('id')

            if complex_id:
                # 2. ë‹¨ì§€ ìƒì„¸ ì •ë³´ ì¡°íšŒ í…ŒìŠ¤íŠ¸
                print("\n" + "=" * 80)
                print("TEST 2: ë‹¨ì§€ ìƒì„¸ ì •ë³´ ì¡°íšŒ")
                print("=" * 80)

                detail = await crawler.get_complex_detail(complex_id)

                print(f"\në‹¨ì§€ëª…: {detail.get('name')}")
                print(f"ì£¼ì†Œ: {detail.get('address')}")
                print(f"ì¤€ê³µë…„ë„: {detail.get('completion_year')}")
                print(f"ì´ ì„¸ëŒ€ìˆ˜: {detail.get('total_households')}")

                # 3. ë§¤ë¬¼ ì •ë³´ ì¡°íšŒ í…ŒìŠ¤íŠ¸
                print("\n" + "=" * 80)
                print("TEST 3: ë§¤ë¬¼ ì •ë³´ ì¡°íšŒ")
                print("=" * 80)

                listings = await crawler.get_listings(complex_id, "ë§¤ë§¤")

                if listings:
                    print(f"\nâœ… ë§¤ë¬¼ {len(listings)}ê°œ ë°œê²¬:")
                    for i, listing in enumerate(listings[:5], 1):  # ìµœëŒ€ 5ê°œë§Œ ì¶œë ¥
                        print(f"\n{i}. ê°€ê²©: {listing.get('price')}")
                        print(f"   ë©´ì : {listing.get('area')}")
                        print(f"   ì¸µ: {listing.get('floor')}")
                else:
                    print("âš ï¸  ë§¤ë¬¼ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

                # 4. ì‹¤ê±°ë˜ê°€ ì¡°íšŒ í…ŒìŠ¤íŠ¸
                print("\n" + "=" * 80)
                print("TEST 4: ì‹¤ê±°ë˜ê°€ ì¡°íšŒ")
                print("=" * 80)

                transactions = await crawler.get_transactions(complex_id)

                if transactions:
                    print(f"\nâœ… ì‹¤ê±°ë˜ {len(transactions)}ê°œ ë°œê²¬:")
                    for i, trans in enumerate(transactions[:5], 1):  # ìµœëŒ€ 5ê°œë§Œ ì¶œë ¥
                        print(f"\n{i}. ê±°ë˜ì¼: {trans.get('date')}")
                        print(f"   ê°€ê²©: {trans.get('price')}")
                        print(f"   ë©´ì : {trans.get('area')}")
                        print(f"   ì¸µ: {trans.get('floor')}")
                else:
                    print("âš ï¸  ì‹¤ê±°ë˜ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

                # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
                await crawler.take_screenshot("test_final_page.png")

            else:
                print("âŒ ë‹¨ì§€ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        else:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

    print("\n" + "=" * 80)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 80)


async def test_specific_complex():
    """íŠ¹ì • ë‹¨ì§€ IDë¡œ ì§ì ‘ í…ŒìŠ¤íŠ¸ (ë‹¨ì§€ IDë¥¼ ì•Œê³  ìˆì„ ë•Œ)"""

    print("=" * 80)
    print("ğŸ§ª íŠ¹ì • ë‹¨ì§€ ì§ì ‘ ì¡°íšŒ í…ŒìŠ¤íŠ¸")
    print("=" * 80)

    # ì˜ˆì‹œ ë‹¨ì§€ ID (ì‹¤ì œë¡œëŠ” ê²€ìƒ‰ì„ í†µí•´ ì–»ì–´ì•¼ í•¨)
    # complex_id = "12345"  # ì‹¤ì œ ë‹¨ì§€ IDë¡œ ë³€ê²½

    # async with NaverRealEstateCrawler(headless=False) as crawler:
    #     detail = await crawler.get_complex_detail(complex_id)
    #     print(json.dumps(detail, ensure_ascii=False, indent=2))

    print("âš ï¸  ì´ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ì‹¤ì œ ë‹¨ì§€ IDê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    print("ë¨¼ì € test_crawler()ë¥¼ ì‹¤í–‰í•˜ì—¬ ë‹¨ì§€ IDë¥¼ ì°¾ìœ¼ì„¸ìš”.")


if __name__ == "__main__":
    print("\nì–´ë–¤ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
    print("1. ì „ì²´ í…ŒìŠ¤íŠ¸ (ë‹¨ì§€ ê²€ìƒ‰ â†’ ìƒì„¸ ì •ë³´ â†’ ë§¤ë¬¼ â†’ ì‹¤ê±°ë˜ê°€)")
    print("2. íŠ¹ì • ë‹¨ì§€ ì§ì ‘ ì¡°íšŒ")
    print()

    choice = input("ì„ íƒ (1 or 2): ").strip()

    if choice == "1":
        asyncio.run(test_crawler())
    elif choice == "2":
        asyncio.run(test_specific_complex())
    else:
        print("1 ë˜ëŠ” 2ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
