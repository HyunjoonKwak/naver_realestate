"""
ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ë§ - Route ë°©ì‹
Request/Responseë¥¼ ê°€ë¡œì±„ì„œ ë°ì´í„° ì¶”ì¶œ
"""
import asyncio
import json
from playwright.async_api import async_playwright


async def crawl_with_route(url: str):
    """Routeë¥¼ ì‚¬ìš©í•œ API ì‘ë‹µ ìº¡ì²˜"""

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        page = await browser.new_page()

        print("=" * 80)
        print("ğŸ•µï¸  ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ë§ (Route ë°©ì‹)")
        print("=" * 80)
        print(f"\nğŸ“ URL: {url}")

        captured_data = []

        # Routeë¡œ ìš”ì²­/ì‘ë‹µ ê°€ë¡œì±„ê¸°
        async def handle_route(route):
            """ëª¨ë“  ìš”ì²­ì„ ê°€ë¡œì±„ì„œ ì‘ë‹µ ìº¡ì²˜"""
            request = route.request
            req_url = request.url

            # ìš”ì²­ ê³„ì† ì§„í–‰
            response = await route.fetch()

            # API ì‘ë‹µ í™•ì¸
            if '/api/' in req_url and response.status == 200:
                try:
                    body_bytes = await response.body()
                    body_text = body_bytes.decode('utf-8')

                    # JSON íŒŒì‹± ì‹œë„
                    try:
                        data = json.loads(body_text)

                        print(f"\nâœ… API ìº¡ì²˜!")
                        print(f"   URL: {req_url.split('?')[0]}")
                        print(f"   Method: {request.method}")

                        captured_data.append({
                            'url': req_url,
                            'method': request.method,
                            'status': response.status,
                            'data': data
                        })

                        # íŒŒì¼ëª… ê²°ì •
                        if 'complexes/' in req_url and req_url.split('/')[-1].split('?')[0].isdigit():
                            filename = 'api_complex_detail.json'
                            print(f"   ğŸ“¦ íƒ€ì…: ë‹¨ì§€ ìƒì„¸")
                        elif 'article' in req_url.lower():
                            filename = 'api_articles.json'
                            print(f"   ğŸ“¦ íƒ€ì…: ë§¤ë¬¼")
                        elif 'price' in req_url.lower() or 'pyoung' in req_url.lower():
                            filename = 'api_prices.json'
                            print(f"   ğŸ“¦ íƒ€ì…: ê°€ê²©")
                        elif 'trade' in req_url.lower():
                            filename = 'api_trades.json'
                            print(f"   ğŸ“¦ íƒ€ì…: ì‹¤ê±°ë˜")
                        else:
                            parts = req_url.split('/')
                            filename = f"api_{parts[-1][:30].replace('?', '_')}.json"
                            print(f"   ğŸ“¦ íƒ€ì…: ê¸°íƒ€")

                        # ì €ì¥
                        with open(filename, 'w', encoding='utf-8') as f:
                            json.dump(data, f, ensure_ascii=False, indent=2)
                        print(f"   ğŸ’¾ ì €ì¥: {filename}")

                        # í‚¤ ì¶œë ¥
                        if isinstance(data, dict):
                            keys = list(data.keys())[:10]
                            print(f"   ğŸ”‘ í‚¤: {keys}")
                        elif isinstance(data, list):
                            print(f"   ğŸ“Š ë°°ì—´ ê¸¸ì´: {len(data)}")

                    except json.JSONDecodeError:
                        # JSONì´ ì•„ë‹Œ ì‘ë‹µ
                        pass

                except Exception as e:
                    print(f"   âš ï¸  ì—ëŸ¬: {e}")

            # ì‘ë‹µ ì „ë‹¬
            await route.fulfill(response=response)

        # ëª¨ë“  ìš”ì²­ì— ëŒ€í•´ route ì„¤ì •
        await page.route("**/*", handle_route)

        # í˜ì´ì§€ ë¡œë“œ
        print("\nâ³ í˜ì´ì§€ ë¡œë”©...")
        await page.goto(url, wait_until="networkidle")
        await asyncio.sleep(5)

        # ìŠ¤í¬ë¡¤
        print("\nğŸ“œ ìŠ¤í¬ë¡¤...")
        for i in range(3):
            await page.evaluate('window.scrollBy(0, 500)')
            await asyncio.sleep(1)

        # ëª¨ë“  í´ë¦­ ê°€ëŠ¥í•œ ìš”ì†Œ ì°¾ê¸°
        print("\nğŸ–±ï¸  í´ë¦­ ê°€ëŠ¥í•œ ìš”ì†Œ ì°¾ê¸°...")
        clickables = await page.query_selector_all('a, button, [role="tab"], [role="button"]')
        print(f"   ë°œê²¬: {len(clickables)}ê°œ")

        # í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ íƒ­ í´ë¦­
        keywords = ['ë§¤ë¬¼', 'ì‹œì„¸', 'ì‹¤ê±°ë˜', 'ê±°ë˜', 'ì •ë³´', 'ë‹¨ì§€']

        for elem in clickables[:30]:
            try:
                text = await elem.inner_text()
                if text and any(kw in text for kw in keywords):
                    print(f"\n   ğŸ–±ï¸  '{text.strip()[:20]}' í´ë¦­...")
                    await elem.click()
                    await asyncio.sleep(2)
            except:
                pass

        # ìŠ¤í¬ë¦°ìƒ·
        await page.screenshot(path="crawled_page.png", full_page=True)
        print("\nğŸ“¸ ìŠ¤í¬ë¦°ìƒ·: crawled_page.png")

        # ëŒ€ê¸°
        print("\nâ¸ï¸  30ì´ˆ ëŒ€ê¸°... (ìˆ˜ë™ìœ¼ë¡œ íƒ­ì„ í´ë¦­í•´ë³´ì„¸ìš”!)")
        await asyncio.sleep(30)

        await browser.close()

        # ê²°ê³¼
        print("\n" + "=" * 80)
        print("ğŸ“Š ê²°ê³¼")
        print("=" * 80)

        if captured_data:
            print(f"\nâœ… {len(captured_data)}ê°œì˜ API ì‘ë‹µ ìº¡ì²˜!")

            # ì „ì²´ ì €ì¥
            with open("all_captured.json", "w", encoding='utf-8') as f:
                json.dump(captured_data, f, ensure_ascii=False, indent=2)
            print("ğŸ’¾ ì „ì²´: all_captured.json")

            # ë°ì´í„° ë¶„ì„
            print("\nğŸ“‹ ìº¡ì²˜ëœ ë°ì´í„°:")
            for item in captured_data:
                url_short = item['url'].split('/')[-1].split('?')[0]
                print(f"   - {item['method']} {url_short}")

                data = item['data']

                # ë‹¨ì§€ ì •ë³´
                if isinstance(data, dict):
                    if 'complexName' in data:
                        print(f"      ë‹¨ì§€: {data['complexName']}")
                    if 'address' in data:
                        print(f"      ì£¼ì†Œ: {data['address']}")
                    if 'articleList' in data:
                        print(f"      ë§¤ë¬¼: {len(data['articleList'])}ê±´")
                    if 'priceList' in data or 'pyoungList' in data:
                        print(f"      ê°€ê²©ì •ë³´ ìˆìŒ")

        else:
            print("\nâŒ ìº¡ì²˜ ì‹¤íŒ¨")

        print("\nâœ… ì™„ë£Œ!")


if __name__ == "__main__":
    target_url = "https://new.land.naver.com/complexes/678?ms=37.5279559,126.895385,19&a=APT:PRE:ABYG:JGC&e=RETAIL"
    asyncio.run(crawl_with_route(target_url))
