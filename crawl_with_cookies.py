"""
ì¿ í‚¤ ë° í—¤ë” ì„¤ì • í›„ í¬ë¡¤ë§
Bot íƒì§€ë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•œ ì„¤ì • ì¶”ê°€
"""
import asyncio
import json
from playwright.async_api import async_playwright


async def crawl_with_proper_setup():
    """ì œëŒ€ë¡œ ì„¤ì •ëœ ë¸Œë¼ìš°ì €ë¡œ í¬ë¡¤ë§"""

    async with async_playwright() as p:
        # ì‹¤ì œ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ë„ë¡ ì„¤ì •
        browser = await p.chromium.launch(
            headless=False,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox'
            ]
        )

        # Context ìƒì„± (User-Agent ë“± ì„¤ì •)
        context = await browser.new_context(
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            viewport={'width': 1920, 'height': 1080},
            locale='ko-KR',
            timezone_id='Asia/Seoul'
        )

        # WebDriver íƒì§€ ìš°íšŒ
        await context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)

        page = await context.new_page()

        print("=" * 80)
        print("ğŸ¢ ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ë§ (Bot íƒì§€ ìš°íšŒ)")
        print("=" * 80)

        target_url = "https://new.land.naver.com/complexes/109208"

        print(f"\nğŸ“ URL: {target_url}")

        captured_data = []

        # Route ì„¤ì •
        async def handle_route(route):
            request = route.request
            req_url = request.url

            response = await route.fetch()

            if '/api/' in req_url:
                try:
                    body_bytes = await response.body()
                    body_text = body_bytes.decode('utf-8')
                    data = json.loads(body_text)

                    print(f"\nâœ… API ìº¡ì²˜: {req_url.split('/')[-1][:50]}")

                    captured_data.append({
                        'url': req_url,
                        'data': data
                    })

                    # ì €ì¥
                    filename = f"api_{len(captured_data)}.json"
                    with open(filename, 'w', encoding='utf-8') as f:
                        json.dump(data, f, ensure_ascii=False, indent=2)

                    # ë°ì´í„° íƒ€ì… í™•ì¸
                    if isinstance(data, dict):
                        if 'complexName' in data:
                            print(f"   ğŸ¢ ë‹¨ì§€: {data['complexName']}")
                        if 'articleList' in data:
                            print(f"   ğŸ’° ë§¤ë¬¼: {len(data['articleList'])}ê±´")
                        print(f"   ğŸ”‘ í‚¤: {list(data.keys())[:5]}")

                except:
                    pass

            await route.fulfill(response=response)

        await page.route("**/*", handle_route)

        # ë¨¼ì € ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ (ì¿ í‚¤ íšë“)
        print("\n1ï¸âƒ£  ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ (ì¿ í‚¤ íšë“)...")
        await page.goto("https://new.land.naver.com/", wait_until="networkidle")
        await asyncio.sleep(3)

        # ì¿ í‚¤ í™•ì¸
        cookies = await context.cookies()
        print(f"   âœ… ì¿ í‚¤ íšë“: {len(cookies)}ê°œ")

        # ì´ì œ ì‹¤ì œ ë‹¨ì§€ í˜ì´ì§€ ë°©ë¬¸
        print(f"\n2ï¸âƒ£  ë‹¨ì§€ í˜ì´ì§€ ë°©ë¬¸...")
        await page.goto(target_url, wait_until="networkidle")
        await asyncio.sleep(5)

        # í˜ì´ì§€ ë‚´ìš© í™•ì¸
        text = await page.evaluate('() => document.body.innerText')
        print(f"\nğŸ“„ í˜ì´ì§€ ë‚´ìš© (ì²˜ìŒ 300ì):")
        print(text[:300])

        # 404ì¸ì§€ í™•ì¸
        if "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in text or "404" in text:
            print("\nâš ï¸  404 í˜ì´ì§€ì…ë‹ˆë‹¤!")
            print("   URLì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
            print("\nğŸ’¡ í•´ê²° ë°©ë²•:")
            print("   1. ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ ë„¤ì´ë²„ ë¶€ë™ì‚° ì ‘ì†")
            print("   2. ë‹¨ì§€ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì°¾ê¸°")
            print("   3. ë‹¨ì§€ í˜ì´ì§€ URL ë³µì‚¬")
            print("   4. ë³µì‚¬í•œ URL ì œê³µ")
        else:
            print("\nâœ… í˜ì´ì§€ ë¡œë“œ ì„±ê³µ!")

            # ìŠ¤í¬ë¡¤ ë° íƒ­ í´ë¦­
            print("\n3ï¸âƒ£  ì¶”ê°€ ë°ì´í„° ë¡œë“œ...")

            # ìŠ¤í¬ë¡¤
            for i in range(5):
                await page.evaluate('window.scrollBy(0, 300)')
                await asyncio.sleep(1)

            # íƒ­ í´ë¦­
            buttons = await page.query_selector_all('button, [role="tab"], a')
            for btn in buttons[:30]:
                try:
                    text = await btn.inner_text()
                    if text and any(kw in text for kw in ['ë§¤ë¬¼', 'ì‹œì„¸', 'ì‹¤ê±°ë˜', 'ê±°ë˜', 'ì •ë³´']):
                        print(f"   ğŸ–±ï¸  '{text.strip()}' í´ë¦­...")
                        await btn.click()
                        await asyncio.sleep(3)
                except:
                    pass

        # ìŠ¤í¬ë¦°ìƒ·
        await page.screenshot(path="final_page.png", full_page=True)
        print("\nğŸ“¸ ìŠ¤í¬ë¦°ìƒ·: final_page.png")

        # ëŒ€ê¸°
        print("\nâ¸ï¸  60ì´ˆ ëŒ€ê¸° (ìˆ˜ë™ ì¡°ì‘ ê°€ëŠ¥)...")
        await asyncio.sleep(60)

        await browser.close()

        # ê²°ê³¼
        print("\n" + "=" * 80)
        print("ğŸ“Š ê²°ê³¼")
        print("=" * 80)

        if captured_data:
            print(f"\nâœ… {len(captured_data)}ê°œ API ì‘ë‹µ ìº¡ì²˜!")

            with open("all_captured.json", "w", encoding='utf-8') as f:
                json.dump(captured_data, f, ensure_ascii=False, indent=2)

            for i, item in enumerate(captured_data, 1):
                print(f"\n{i}. {item['url'].split('/')[-1][:50]}")
                data = item['data']
                if isinstance(data, dict):
                    print(f"   í‚¤: {list(data.keys())[:10]}")
        else:
            print("\nâŒ ìº¡ì²˜ ì‹¤íŒ¨")

        print("\nâœ… ì™„ë£Œ!")


if __name__ == "__main__":
    asyncio.run(crawl_with_proper_setup())
