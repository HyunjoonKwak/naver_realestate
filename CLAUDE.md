# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Korean real estate tracking system that crawls Naver Real Estate for apartment complex listings, monitors price changes, and provides analytics. Full-stack application with FastAPI backend, Next.js 14 frontend, PostgreSQL database, and Playwright-based crawler.

**Tech Stack:** Python 3.13, FastAPI, PostgreSQL 15, Redis 7, SQLAlchemy 2.0, Next.js 14 (App Router), TypeScript, Tailwind CSS

## Essential Commands

### Initial Setup
```bash
# Start Docker containers (PostgreSQL + Redis)
docker-compose up -d

# Migrate database with foreign keys (recommended - Phase 1 improvement)
backend/.venv/bin/python migrate_db.py

# Legacy: Initialize/reset database tables (no foreign keys)
backend/.venv/bin/python reset_db.py

# Install Playwright browsers (first time only)
backend/.venv/bin/playwright install chromium

# Install frontend dependencies (first time only)
cd frontend && npm install
```

### Development
```bash
# Start API server (from project root)
cd backend
.venv/bin/uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

# Start frontend dev server (from project root)
cd frontend
npm run dev

# API documentation
open http://localhost:8000/docs
```

### Testing
```bash
# Test API endpoints
./test_api.sh
```

## Architecture

### Backend Structure (FastAPI)

**Core Application:** [backend/app/main.py](backend/app/main.py) - Main FastAPI app with CORS middleware and router registration

**Database Layer:**
- [backend/app/core/database.py](backend/app/core/database.py) - SQLAlchemy engine/session management, `get_db()` dependency
- [backend/app/models/complex.py](backend/app/models/complex.py) - All database models using declarative base:
  - `Complex` - Apartment complex master data
  - `Article` - Current listings (unique by `article_no`)
  - `Transaction` - Real transaction history
  - `ArticleSnapshot` - Point-in-time article states for change tracking
  - `ArticleChange` - Detected changes (NEW/REMOVED/PRICE_UP/PRICE_DOWN)
  - `ArticleHistory` - Legacy change tracking

**API Routes:** [backend/app/api/](backend/app/api/)
- `complexes.py` - Complex CRUD, stats, associated articles/transactions
- `articles.py` - Article listing, filtering, price change queries
- `scraper.py` - Background crawling endpoints:
  - POST /api/scraper/crawl (body: {complex_id})
  - POST /api/scraper/crawl/{complex_id} (RESTful path param)
  - POST /api/scraper/refresh/{complex_id} (with change tracking)
- `transactions.py` - Transaction queries and analytics

**Schemas:** [backend/app/schemas/](backend/app/schemas/) - Pydantic models for request/response validation

**Services:** [backend/app/services/](backend/app/services/)
- `crawler_service.py` - NaverRealEstateCrawler class (moved from root)
- `article_tracker.py` - Article change detection and snapshot management
- `molit_service.py` - êµ­í† êµí†µë¶€ ì‹¤ê±°ë˜ê°€ API ì—°ë™ (XML íŒŒì‹±, í˜ì´ì§€ë„¤ì´ì…˜)
- `transaction_service.py` - ì‹¤ê±°ë˜ê°€ ë°ì´í„° ì €ì¥ ë° í†µê³„ ì²˜ë¦¬
- `location_parser.py` - ë²•ì •ë™ ì½”ë“œ íŒŒì‹± ë° ì‹œêµ°êµ¬ ì½”ë“œ ì¶”ì¶œ (20,000ê°œ ë²•ì •ë™)

### Crawler Architecture

**Main Crawler:** [backend/app/services/crawler_service.py](backend/app/services/crawler_service.py) - Playwright-based scraper with critical bot evasion features:

1. **ë™ì¼ë§¤ë¬¼ë¬¶ê¸° (Same Address Grouping):** Sets `localStorage.sameAddrYn=true` BEFORE page load, clicks checkbox, validates `sameAddressGroup=true` in API calls. This groups duplicate listings by same address.

2. **Network Response Interception:** Captures Naver API responses for complex data, article lists, and transactions during page interaction.

3. **Scroll-based Collection:** Scrolls `.item_list` container to trigger lazy loading and collect all listings.

4. **Incremental Updates:** Compares with existing DB records by `article_no`, detects NEW/REMOVED/PRICE_CHANGED items, stores snapshots.

5. **Background Execution:** Triggered via `/api/scrape/{complex_id}` endpoint for async processing.

### Frontend Structure (Next.js App Router)

**Pages:** [frontend/src/app/](frontend/src/app/)
- `page.tsx` - Dashboard with complex grid and stats
- `complexes/page.tsx` - Complex list page
- `complexes/[id]/page.tsx` - Complex detail with articles table, filters, price cards
- `complexes/new/page.tsx` - Add new complex by Naver URL

**API Client:** [frontend/src/lib/api.ts](frontend/src/lib/api.ts) - Axios-based client for backend API calls

**Types:** [frontend/src/types/](frontend/src/types/) - TypeScript interfaces matching backend schemas

### Database Design

**Key Relationships (Phase 1: Foreign Keys Added):**
- `Complex.complex_id` â†’ `Article.complex_id` (1:N, CASCADE delete)
- `Complex.complex_id` â†’ `Transaction.complex_id` (1:N, CASCADE delete)
- `Complex.complex_id` â†’ `ArticleSnapshot.complex_id` (1:N, CASCADE delete)
- `Complex.complex_id` â†’ `ArticleChange.complex_id` (1:N, CASCADE delete)
- `Article.article_no` â†’ `ArticleSnapshot.article_no` (1:N for history)
- `ArticleChange` references snapshots via `from_snapshot_id` / `to_snapshot_id`

**Important Fields:**
- `Article.same_addr_cnt` - Number of realtors with same listing (from ë™ì¼ë§¤ë¬¼ë¬¶ê¸°)
- `Article.price_change_state` - "SAME", "UP", "DOWN" from Naver API
- `Article.is_active` - False when listing removed from site
- `ArticleChange.change_type` - "NEW", "REMOVED", "PRICE_UP", "PRICE_DOWN"

### Data Flow

1. **Adding Complex:** Frontend sends Naver URL â†’ POST /api/scrape/{complex_id} â†’ Crawler runs in background â†’ Stores Complex + Articles in DB
2. **Viewing Articles:** Frontend calls GET /api/complexes/{complex_id}/articles â†’ Returns filtered articles with stats
3. **Change Detection:** Crawler compares new snapshot with latest ArticleSnapshot â†’ Creates ArticleChange records â†’ Used for weekly briefing feature

## Critical Implementation Details

**Bot Evasion (DO NOT MODIFY):** Crawler uses these techniques to avoid Naver bot detection:
- `headless=False` - Required for bypassing bot detection
- `--disable-blink-features=AutomationControlled` - Hides automation signals
- `slow_mo=100` - Natural-looking browser actions
- `await asyncio.sleep(1.5)` - Scroll speed throttling
- localStorage setup BEFORE page navigation

**Crawler localStorage Setup:** Must inject `localStorage.setItem('sameAddrYn', 'true')` via `page.add_init_script()` BEFORE navigating to Naver page. Checkbox clicking alone is insufficient.

**Article Deduplication:** Use `article_no` as unique identifier, not realtor name or price combinations. Multiple realtors can list same article.

**Database Sessions:** Always use `get_db()` dependency in FastAPI routes. Crawler service accepts optional `db` parameter or creates `SessionLocal()`.

**Change Tracking (Phase 1):** All crawling endpoints now enable snapshot creation and change detection by default (`create_snapshot=True`).

**Foreign Keys (Phase 1):** Deleting a Complex now CASCADE deletes all Articles, Transactions, Snapshots, and Changes. Use `migrate_db.py` to apply.

**CORS Configuration:** Backend allows all origins (`*`) - restrict in production.

**Complex ID Extraction:** From Naver URL `https://new.land.naver.com/complexes/{complex_id}`, extract numeric ID.

**Real Transaction Integration (NEW):** Refresh endpoint automatically fetches MOLIT data after crawling. See `docs/TRANSACTION_GUIDE.md` for setup.

## Real Transaction (ì‹¤ê±°ë˜ê°€) Feature

### Setup
1. **Get API Key**: https://www.data.go.kr/data/15058017/openapi.do
2. **Add to .env**: `MOLIT_API_KEY=your_key_here`
3. **Restart server**: `.venv/bin/uvicorn app.main:app --reload`

### Architecture
- **Data Source**: êµ­í† êµí†µë¶€ ê³µê³µë°ì´í„° (XML format)
- **Coverage**: ì „êµ­ 20,278ê°œ ë²•ì •ë™ ì½”ë“œ ìë™ ë§¤ì¹­
- **Auto-fetch**: ë‹¨ì§€ ìƒˆë¡œê³ ì¹¨ ì‹œ ìë™ìœ¼ë¡œ ìµœê·¼ 6ê°œì›” ì‹¤ê±°ë˜ê°€ ì¡°íšŒ
- **Deduplication**: ê°™ì€ ë‚ ì§œ/ë©´ì /ì¸µ/ê°€ê²© ê±°ë˜ëŠ” ì¤‘ë³µ ì œê±°

### Services
1. **MOLITService** ([molit_service.py](backend/app/services/molit_service.py))
   - XML ì‘ë‹µ íŒŒì‹± (`_parse_xml_response()`)
   - í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬ (`_fetch_all_pages()`) - 1000ê±´ ì´ìƒ ìë™ ìˆ˜ì§‘
   - ë§¤ë§¤/ì „ì›”ì„¸ API ì§€ì›
   - LocationParser í†µí•©ìœ¼ë¡œ ì „êµ­ ì£¼ì†Œ ìë™ ë§¤ì¹­

2. **TransactionService** ([transaction_service.py](backend/app/services/transaction_service.py))
   - `fetch_and_save_transactions()` - API ì¡°íšŒ ë° DB ì €ì¥
   - `get_area_stats()` - í‰í˜•ë³„ í†µê³„ ê³„ì‚° (í‰ê· /ìµœê³ /ìµœì €/ê±°ë˜ê±´ìˆ˜)
   - ê°€ê²© í¬ë§·íŒ… (ì–µ/ë§Œì›)

3. **LocationParser** ([location_parser.py](backend/app/services/location_parser.py))
   - ë²•ì •ë™ ì½”ë“œ íŒŒì¼ ë¡œë“œ (`dong_code_active.txt`, 20,278ê°œ)
   - ì£¼ì†Œ â†’ ì‹œêµ°êµ¬ ì½”ë“œ(5ìë¦¬) ìë™ ì¶”ì¶œ
   - ì •í™•í•œ ë§¤ì¹­ + ë¶€ë¶„ ë§¤ì¹­ + fallback ì „ëµ

### API Endpoints
- `POST /api/transactions/fetch/{complex_id}` - ì‹¤ê±°ë˜ê°€ ìˆ˜ë™ ì¡°íšŒ
- `GET /api/transactions/stats/area-summary/{complex_id}` - í‰í˜•ë³„ í†µê³„
- `POST /api/scraper/refresh/{complex_id}` - í¬ë¡¤ë§ + ì‹¤ê±°ë˜ê°€ ìë™ ì¡°íšŒ

### Frontend Integration
- **ìœ„ì¹˜**: ë‹¨ì§€ ìƒì„¸ í˜ì´ì§€ ([complexes/[id]/page.tsx](frontend/src/app/complexes/[id]/page.tsx))
- **UI**: ë³€ë™ì‚¬í•­ ìš”ì•½ ì•„ë˜ "ğŸ’° ì‹¤ê±°ë˜ê°€ ìš”ì•½" ì„¹ì…˜
- **í‘œì‹œ ì •ë³´**: í‰í˜•ë³„ ì¹´ë“œ (í‰ê· /ìµœê³ /ìµœì €ê°€, ê±°ë˜ê±´ìˆ˜)
- **ìë™ ì—…ë°ì´íŠ¸**: ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼ í´ë¦­ ì‹œ ìë™ ì¡°íšŒ

### Testing
```bash
# í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
./test_transaction_api.sh

# LocationParser í…ŒìŠ¤íŠ¸
cd backend && .venv/bin/python
>>> from app.services.location_parser import LocationParser
>>> parser = LocationParser()
>>> parser.extract_sigungu_code("ê²½ê¸°ë„ ì„±ë‚¨ì‹œ ë¶„ë‹¹êµ¬")
'41135'
```

### Data Files
- **ë²•ì •ë™ ì½”ë“œ**: `backend/app/data/dong_code_active.txt` (20,278 records)
- **Format**: `ë²•ì •ë™ì½”ë“œ\të²•ì •ë™ëª…` (TSV)

## Known Limitations

- No user authentication system yet (planned)
- No scheduled crawling (Celery integration planned)
- ~~Real transaction (ì‹¤ê±°ë˜ê°€) feature was removed~~ â†’ **âœ… Re-implemented (Phase 2)**
- Weekly briefing feature designed but not fully implemented
- Crawling requires headless=False to avoid bot detection
- MOLIT API has rate limits (1,000 calls/day for general key)
