"""
ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ë‹¨ì§€ ID ì°¾ê¸°
ë„¤ì´ë²„ ë¶€ë™ì‚° ì§€ë„ì—ì„œ ì‹¤ì œ ë‹¨ì§€ í´ë¦­
"""
import asyncio
from playwright.async_api import async_playwright


async def find_complex_ids():
    """ì§€ë„ì—ì„œ ì‹¤ì œ ë‹¨ì§€ í´ë¦­í•˜ì—¬ ID ì°¾ê¸°"""

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        page = await browser.new_page()

        print("=" * 80)
        print("ğŸ—ºï¸  ì‹¤ì œ ë‹¨ì§€ ID ì°¾ê¸°")
        print("=" * 80)

        # ë„¤ì´ë²„ ë¶€ë™ì‚° ë©”ì¸ (ì„œìš¸ ê°•ë‚¨ ì§€ì—­)
        url = "https://new.land.naver.com/complexes?ms=37.498095,127.027610,16&a=APT&e=RETAIL"

        print(f"\nğŸ“ URL: {url}")
        await page.goto(url, wait_until="networkidle")

        print("\nâ³ ì§€ë„ ë¡œë”© ëŒ€ê¸° ì¤‘...")
        await asyncio.sleep(10)

        # í˜ì´ì§€ì— ìˆëŠ” ë§í¬ë“¤ ì°¾ê¸°
        print("\n" + "=" * 80)
        print("ğŸ”— ë‹¨ì§€ ë§í¬ ì°¾ê¸°")
        print("=" * 80)

        # complexes/ìˆ«ì íŒ¨í„´ì˜ ë§í¬ ì°¾ê¸°
        links = await page.evaluate('''
            () => {
                const allLinks = Array.from(document.querySelectorAll('a[href*="/complexes/"]'));
                return allLinks.map(link => ({
                    href: link.href,
                    text: link.textContent.trim()
                })).filter(item => item.href.match(/complexes\\/\\d+/));
            }
        ''')

        if links:
            print(f"âœ… {len(links)}ê°œì˜ ë‹¨ì§€ ë§í¬ ë°œê²¬!")

            # ì¤‘ë³µ ì œê±°
            unique_links = {}
            for link in links:
                unique_links[link['href']] = link['text']

            print(f"\nğŸ“‹ ì‹¤ì œ ë‹¨ì§€ ëª©ë¡ (ì¤‘ë³µ ì œê±°):")
            for i, (href, text) in enumerate(list(unique_links.items())[:10], 1):
                # URLì—ì„œ ë‹¨ì§€ ID ì¶”ì¶œ
                import re
                match = re.search(r'complexes/(\\d+)', href)
                complex_id = match.group(1) if match else "Unknown"

                print(f"\n{i}. ë‹¨ì§€ ID: {complex_id}")
                print(f"   ì´ë¦„: {text[:50]}")
                print(f"   URL: {href}")

            # ì²« ë²ˆì§¸ ë‹¨ì§€ ë°©ë¬¸
            if unique_links:
                first_url = list(unique_links.keys())[0]
                print(f"\n" + "=" * 80)
                print(f"ğŸ¢ ì²« ë²ˆì§¸ ë‹¨ì§€ ë°©ë¬¸: {first_url}")
                print("=" * 80)

                await page.goto(first_url, wait_until="networkidle")
                await asyncio.sleep(5)

                # í˜ì´ì§€ í…ìŠ¤íŠ¸ í™•ì¸
                text_content = await page.evaluate('() => document.body.innerText')
                print(f"\ní˜ì´ì§€ ë‚´ìš© (ì²˜ìŒ 500ì):")
                print(text_content[:500])

                # ìŠ¤í¬ë¦°ìƒ·
                await page.screenshot(path="real_complex_page.png", full_page=True)
                print(f"\nğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: real_complex_page.png")

        else:
            print("âŒ ë‹¨ì§€ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

            # í˜ì´ì§€ì˜ ëª¨ë“  ë§í¬ ì¶œë ¥
            all_links = await page.evaluate('() => Array.from(document.querySelectorAll("a")).map(a => a.href)')
            print(f"\ní˜ì´ì§€ì˜ ëª¨ë“  ë§í¬ ({len(all_links)}ê°œ):")
            for link in all_links[:20]:
                print(f"  - {link}")

        print("\nâ¸ï¸  ë¸Œë¼ìš°ì €ë¥¼ 60ì´ˆê°„ ì—´ì–´ë‘¡ë‹ˆë‹¤. ì§ì ‘ ë‹¨ì§€ë¥¼ í´ë¦­í•´ë³´ì„¸ìš”!")
        print("  í´ë¦­í•œ ë‹¨ì§€ì˜ URLì„ ë³µì‚¬í•˜ì„¸ìš”!")
        await asyncio.sleep(60)

        await browser.close()

        print("\nâœ… ì™„ë£Œ!")


if __name__ == "__main__":
    asyncio.run(find_complex_ids())
