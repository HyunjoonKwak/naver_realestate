"""
ìë™ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ íƒœìŠ¤í¬
"""
import asyncio
import logging
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from app.core.celery_app import celery_app
from app.core.database import SessionLocal
from app.models.complex import Complex, ArticleSnapshot
from app.services.crawler_service import NaverRealEstateCrawler

logger = logging.getLogger(__name__)


@celery_app.task(name="app.tasks.scheduler.crawl_all_complexes")
def crawl_all_complexes():
    """
    ë“±ë¡ëœ ëª¨ë“  ë‹¨ì§€ë¥¼ í¬ë¡¤ë§í•˜ëŠ” íƒœìŠ¤í¬

    Returns:
        dict: í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½
    """
    logger.info("=" * 80)
    logger.info("ğŸ¤– ìë™ í¬ë¡¤ë§ ì‹œì‘")
    logger.info("=" * 80)

    db = SessionLocal()
    results = {
        "started_at": datetime.now().isoformat(),
        "total_complexes": 0,
        "success": 0,
        "failed": 0,
        "errors": []
    }

    try:
        # ëª¨ë“  ë‹¨ì§€ ì¡°íšŒ
        complexes = db.query(Complex).all()
        results["total_complexes"] = len(complexes)

        logger.info(f"ğŸ“‹ í¬ë¡¤ë§ ëŒ€ìƒ: {len(complexes)}ê°œ ë‹¨ì§€")

        # ê° ë‹¨ì§€ë³„ë¡œ í¬ë¡¤ë§ ì‹¤í–‰
        for idx, complex_obj in enumerate(complexes, 1):
            complex_id = complex_obj.complex_id
            complex_name = complex_obj.complex_name

            try:
                logger.info(f"[{idx}/{len(complexes)}] í¬ë¡¤ë§ ì‹œì‘: {complex_name} (ID: {complex_id})")

                # ë¹„ë™ê¸° í¬ë¡¤ë§ ì‹¤í–‰
                asyncio.run(crawl_single_complex(complex_id, db))

                results["success"] += 1
                logger.info(f"âœ… [{idx}/{len(complexes)}] ì™„ë£Œ: {complex_name}")

            except Exception as e:
                results["failed"] += 1
                error_msg = f"ë‹¨ì§€ {complex_id} ({complex_name}) í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}"
                results["errors"].append(error_msg)
                logger.error(f"âŒ [{idx}/{len(complexes)}] ì‹¤íŒ¨: {complex_name} - {str(e)}")

            # ë‹¨ì§€ ê°„ ë”œë ˆì´ (ë´‡ ì°¨ë‹¨ ë°©ì§€)
            if idx < len(complexes):
                import time
                time.sleep(5)  # 5ì´ˆ ëŒ€ê¸°

        results["finished_at"] = datetime.now().isoformat()

        logger.info("=" * 80)
        logger.info(f"ğŸ ìë™ í¬ë¡¤ë§ ì™„ë£Œ")
        logger.info(f"   ì´ {results['total_complexes']}ê°œ ì¤‘ {results['success']}ê°œ ì„±ê³µ, {results['failed']}ê°œ ì‹¤íŒ¨")
        logger.info("=" * 80)

    except Exception as e:
        logger.error(f"ìë™ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        results["errors"].append(f"ì „ì²´ ì‘ì—… ì˜¤ë¥˜: {str(e)}")
    finally:
        db.close()

    return results


async def crawl_single_complex(complex_id: str, db: Session):
    """
    ë‹¨ì¼ ë‹¨ì§€ í¬ë¡¤ë§ (ë¹„ë™ê¸°)

    Args:
        complex_id: ë‹¨ì§€ ID
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
    """
    crawler = NaverRealEstateCrawler()
    await crawler.crawl_complex(complex_id)


@celery_app.task(name="app.tasks.scheduler.cleanup_old_snapshots")
def cleanup_old_snapshots():
    """
    30ì¼ ì´ìƒ ëœ ì˜¤ë˜ëœ ìŠ¤ëƒ…ìƒ·ì„ ì •ë¦¬í•˜ëŠ” íƒœìŠ¤í¬

    Returns:
        dict: ì •ë¦¬ ê²°ê³¼
    """
    logger.info("=" * 80)
    logger.info("ğŸ§¹ ì˜¤ë˜ëœ ìŠ¤ëƒ…ìƒ· ì •ë¦¬ ì‹œì‘")
    logger.info("=" * 80)

    db = SessionLocal()
    results = {
        "started_at": datetime.now().isoformat(),
        "deleted_count": 0,
        "errors": []
    }

    try:
        # 30ì¼ ì´ì „ ë‚ ì§œ ê³„ì‚°
        cutoff_date = datetime.now() - timedelta(days=30)

        # ì˜¤ë˜ëœ ìŠ¤ëƒ…ìƒ· ì¡°íšŒ
        old_snapshots = db.query(ArticleSnapshot).filter(
            ArticleSnapshot.captured_at < cutoff_date
        ).all()

        logger.info(f"ğŸ“Š ì‚­ì œ ëŒ€ìƒ: {len(old_snapshots)}ê°œ ìŠ¤ëƒ…ìƒ·")

        # ìŠ¤ëƒ…ìƒ· ì‚­ì œ
        for snapshot in old_snapshots:
            db.delete(snapshot)

        db.commit()
        results["deleted_count"] = len(old_snapshots)
        results["finished_at"] = datetime.now().isoformat()

        logger.info(f"âœ… {len(old_snapshots)}ê°œ ìŠ¤ëƒ…ìƒ· ì‚­ì œ ì™„ë£Œ")
        logger.info("=" * 80)

    except Exception as e:
        db.rollback()
        error_msg = f"ìŠ¤ëƒ…ìƒ· ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        results["errors"].append(error_msg)
        logger.error(error_msg)
    finally:
        db.close()

    return results


@celery_app.task(name="app.tasks.scheduler.crawl_complex_async")
def crawl_complex_async(complex_id: str):
    """
    íŠ¹ì • ë‹¨ì§€ë¥¼ ë¹„ë™ê¸°ë¡œ í¬ë¡¤ë§í•˜ëŠ” íƒœìŠ¤í¬ (APIì—ì„œ í˜¸ì¶œìš©)

    Args:
        complex_id: ë‹¨ì§€ ID

    Returns:
        dict: í¬ë¡¤ë§ ê²°ê³¼
    """
    logger.info(f"ğŸ¤– ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì‹œì‘: {complex_id}")

    db = SessionLocal()
    result = {
        "complex_id": complex_id,
        "started_at": datetime.now().isoformat(),
        "success": False,
        "error": None
    }

    try:
        asyncio.run(crawl_single_complex(complex_id, db))
        result["success"] = True
        result["finished_at"] = datetime.now().isoformat()
        logger.info(f"âœ… ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì™„ë£Œ: {complex_id}")
    except Exception as e:
        result["error"] = str(e)
        logger.error(f"âŒ ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì‹¤íŒ¨: {complex_id} - {str(e)}")
    finally:
        db.close()

    return result
