"""
ìë™ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ íƒœìŠ¤í¬
"""
import asyncio
import logging
import uuid
import traceback
from datetime import datetime, timedelta, timezone
from sqlalchemy.orm import Session
from app.core.celery_app import celery_app
from app.core.database import SessionLocal
from app.models.complex import Complex, ArticleSnapshot, CrawlJob
from app.services.crawler_service import NaverRealEstateCrawler

logger = logging.getLogger(__name__)


@celery_app.task(name="app.tasks.scheduler.crawl_all_complexes", bind=True)
def crawl_all_complexes(self, job_type='scheduled'):
    """
    ë“±ë¡ëœ ëª¨ë“  ë‹¨ì§€ë¥¼ í¬ë¡¤ë§í•˜ëŠ” íƒœìŠ¤í¬

    Args:
        job_type: ì‘ì—… ìœ í˜• ('scheduled' ë˜ëŠ” 'manual')

    Returns:
        dict: í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½
    """
    job_id = str(uuid.uuid4())
    db = SessionLocal()

    # CrawlJob ë ˆì½”ë“œ ìƒì„±
    job = CrawlJob(
        job_id=job_id,
        job_type=job_type,
        status='running',
        started_at=datetime.now(timezone.utc),
        celery_task_id=self.request.id
    )
    db.add(job)
    db.commit()

    logger.info("=" * 80)
    logger.info(f"ğŸ¤– ìë™ í¬ë¡¤ë§ ì‹œì‘ (Job ID: {job_id})")
    logger.info("=" * 80)

    results = {
        "job_id": job_id,
        "started_at": job.started_at.isoformat(),
        "total_complexes": 0,
        "success": 0,
        "failed": 0,
        "errors": [],
        "total_articles_collected": 0,
        "total_articles_new": 0,
        "total_articles_updated": 0
    }

    try:
        # ëª¨ë“  ë‹¨ì§€ ì¡°íšŒ
        complexes = db.query(Complex).all()
        results["total_complexes"] = len(complexes)
        job.complex_name = f"ì „ì²´ {len(complexes)}ê°œ ë‹¨ì§€"
        db.commit()

        logger.info(f"ğŸ“‹ í¬ë¡¤ë§ ëŒ€ìƒ: {len(complexes)}ê°œ ë‹¨ì§€")

        # ê° ë‹¨ì§€ë³„ë¡œ í¬ë¡¤ë§ ì‹¤í–‰
        for idx, complex_obj in enumerate(complexes, 1):
            complex_id = complex_obj.complex_id
            complex_name = complex_obj.complex_name

            try:
                logger.info(f"[{idx}/{len(complexes)}] í¬ë¡¤ë§ ì‹œì‘: {complex_name} (ID: {complex_id})")

                # ê°œë³„ ë‹¨ì§€ í¬ë¡¤ë§ (ê²°ê³¼ í¬í•¨)
                crawl_result = asyncio.run(crawl_single_complex_with_result(complex_id, db))

                results["success"] += 1
                results["total_articles_collected"] += crawl_result.get("articles_collected", 0)
                results["total_articles_new"] += crawl_result.get("articles_new", 0)
                results["total_articles_updated"] += crawl_result.get("articles_updated", 0)

                logger.info(f"âœ… [{idx}/{len(complexes)}] ì™„ë£Œ: {complex_name}")

            except Exception as e:
                results["failed"] += 1
                error_msg = f"ë‹¨ì§€ {complex_id} ({complex_name}) í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}"
                results["errors"].append(error_msg)
                logger.error(f"âŒ [{idx}/{len(complexes)}] ì‹¤íŒ¨: {complex_name} - {str(e)}")

            # ë‹¨ì§€ ê°„ ë”œë ˆì´ (ë´‡ ì°¨ë‹¨ ë°©ì§€)
            if idx < len(complexes):
                import time
                time.sleep(5)  # 5ì´ˆ ëŒ€ê¸°

        # ì‘ì—… ì™„ë£Œ ì²˜ë¦¬
        job.status = 'success' if results["failed"] == 0 else 'failed'
        job.finished_at = datetime.now(timezone.utc)
        job.duration_seconds = int((job.finished_at - job.started_at).total_seconds())
        job.articles_collected = results["total_articles_collected"]
        job.articles_new = results["total_articles_new"]
        job.articles_updated = results["total_articles_updated"]

        if results["errors"]:
            job.error_message = "\n".join(results["errors"][:10])  # ìµœëŒ€ 10ê°œë§Œ

        db.commit()

        results["finished_at"] = job.finished_at.isoformat()
        results["duration_seconds"] = job.duration_seconds

        logger.info("=" * 80)
        logger.info(f"ğŸ ìë™ í¬ë¡¤ë§ ì™„ë£Œ")
        logger.info(f"   ì´ {results['total_complexes']}ê°œ ì¤‘ {results['success']}ê°œ ì„±ê³µ, {results['failed']}ê°œ ì‹¤íŒ¨")
        logger.info(f"   ìˆ˜ì§‘: {results['total_articles_collected']}ê±´, ì‹ ê·œ: {results['total_articles_new']}ê±´")
        logger.info("=" * 80)

    except Exception as e:
        logger.error(f"ìë™ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        results["errors"].append(f"ì „ì²´ ì‘ì—… ì˜¤ë¥˜: {str(e)}")

        # ì‘ì—… ì‹¤íŒ¨ ì²˜ë¦¬
        job.status = 'failed'
        job.finished_at = datetime.now(timezone.utc)
        job.duration_seconds = int((job.finished_at - job.started_at).total_seconds())
        job.error_message = str(e)
        job.error_traceback = traceback.format_exc()
        db.commit()

    finally:
        db.close()

    return results


async def crawl_single_complex(complex_id: str, db: Session):
    """
    ë‹¨ì¼ ë‹¨ì§€ í¬ë¡¤ë§ (ë¹„ë™ê¸°)

    Args:
        complex_id: ë‹¨ì§€ ID
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
    """
    crawler = NaverRealEstateCrawler()
    await crawler.crawl_complex(complex_id)


async def crawl_single_complex_with_result(complex_id: str, db: Session):
    """
    ë‹¨ì¼ ë‹¨ì§€ í¬ë¡¤ë§ (ê²°ê³¼ í¬í•¨)

    Args:
        complex_id: ë‹¨ì§€ ID
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜

    Returns:
        dict: í¬ë¡¤ë§ ê²°ê³¼ (articles_collected, articles_new, articles_updated)
    """
    # í¬ë¡¤ë§ ì „ ë§¤ë¬¼ ìˆ˜
    from app.models.complex import Article
    before_count = db.query(Article).filter(Article.complex_id == complex_id).count()

    # í¬ë¡¤ë§ ì‹¤í–‰
    crawler = NaverRealEstateCrawler()
    await crawler.crawl_complex(complex_id)

    # í¬ë¡¤ë§ í›„ ë§¤ë¬¼ ìˆ˜
    after_count = db.query(Article).filter(Article.complex_id == complex_id).count()

    # ê°„ë‹¨í•œ í†µê³„ (ì •í™•í•œ ì‹ ê·œ/ì—…ë°ì´íŠ¸ êµ¬ë¶„ì€ ë³µì¡í•˜ë¯€ë¡œ ê·¼ì‚¬ì¹˜)
    articles_new = max(0, after_count - before_count)

    return {
        "articles_collected": after_count,
        "articles_new": articles_new,
        "articles_updated": min(before_count, after_count)
    }


@celery_app.task(name="app.tasks.scheduler.cleanup_old_snapshots")
def cleanup_old_snapshots():
    """
    90ì¼ ì´ìƒ ëœ ì˜¤ë˜ëœ ìŠ¤ëƒ…ìƒ·ì„ ì •ë¦¬í•˜ëŠ” íƒœìŠ¤í¬ (ë¶„ê¸°ë³„ 1íšŒ ì‹¤í–‰ ê¶Œì¥)

    - ë§¤ë¬¼ ìŠ¤ëƒ…ìƒ·ì´ ê³„ì† ìŒ“ì´ë©´ DB ìš©ëŸ‰ ì¦ê°€
    - 90ì¼(ì•½ 3ê°œì›”) ì´ì „ ìŠ¤ëƒ…ìƒ·ì€ ì‚­ì œí•˜ì—¬ ìš©ëŸ‰ ì ˆì•½
    - ê¶Œì¥ ì‹¤í–‰: ë¶„ê¸°ë³„ 1íšŒ (ì˜ˆ: 1ì›”, 4ì›”, 7ì›”, 10ì›” 1ì¼ ìƒˆë²½ 3ì‹œ)

    Returns:
        dict: ì •ë¦¬ ê²°ê³¼
    """
    logger.info("=" * 80)
    logger.info("ğŸ§¹ ì˜¤ë˜ëœ ìŠ¤ëƒ…ìƒ· ì •ë¦¬ ì‹œì‘")
    logger.info("=" * 80)

    db = SessionLocal()
    results = {
        "started_at": datetime.now(timezone.utc).isoformat(),
        "deleted_count": 0,
        "errors": []
    }

    try:
        # 90ì¼ ì´ì „ ë‚ ì§œ ê³„ì‚° (ë¶„ê¸°ë³„ 1íšŒ ì‹¤í–‰)
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=90)

        # ì˜¤ë˜ëœ ìŠ¤ëƒ…ìƒ· ì¡°íšŒ (snapshot_date ê¸°ì¤€)
        old_snapshots = db.query(ArticleSnapshot).filter(
            ArticleSnapshot.snapshot_date < cutoff_date
        ).all()

        logger.info(f"ğŸ“Š ì‚­ì œ ëŒ€ìƒ: {len(old_snapshots)}ê°œ ìŠ¤ëƒ…ìƒ· (90ì¼ ì´ì „)")

        # ìŠ¤ëƒ…ìƒ· ì‚­ì œ
        if len(old_snapshots) > 0:
            for snapshot in old_snapshots:
                db.delete(snapshot)

            db.commit()
            results["deleted_count"] = len(old_snapshots)
            logger.info(f"âœ… {len(old_snapshots)}ê°œ ìŠ¤ëƒ…ìƒ· ì‚­ì œ ì™„ë£Œ")
        else:
            logger.info("â„¹ï¸  ì‚­ì œí•  ìŠ¤ëƒ…ìƒ·ì´ ì—†ìŠµë‹ˆë‹¤")

        results["finished_at"] = datetime.now(timezone.utc).isoformat()
        logger.info("=" * 80)

    except Exception as e:
        db.rollback()
        error_msg = f"ìŠ¤ëƒ…ìƒ· ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        results["errors"].append(error_msg)
        logger.error(error_msg)
        logger.error(traceback.format_exc())
    finally:
        db.close()

    return results


@celery_app.task(name="app.tasks.scheduler.crawl_complex_async", bind=True)
def crawl_complex_async(self, complex_id: str):
    """
    íŠ¹ì • ë‹¨ì§€ë¥¼ ë¹„ë™ê¸°ë¡œ í¬ë¡¤ë§í•˜ëŠ” íƒœìŠ¤í¬ (APIì—ì„œ í˜¸ì¶œìš©)

    Args:
        complex_id: ë‹¨ì§€ ID

    Returns:
        dict: í¬ë¡¤ë§ ê²°ê³¼
    """
    job_id = str(uuid.uuid4())
    db = SessionLocal()

    # ë‹¨ì§€ ì •ë³´ ì¡°íšŒ
    complex_obj = db.query(Complex).filter(Complex.complex_id == complex_id).first()
    complex_name = complex_obj.complex_name if complex_obj else complex_id

    # CrawlJob ë ˆì½”ë“œ ìƒì„±
    job = CrawlJob(
        job_id=job_id,
        job_type='manual',
        complex_id=complex_id,
        complex_name=complex_name,
        status='running',
        started_at=datetime.now(timezone.utc),
        celery_task_id=self.request.id
    )
    db.add(job)
    db.commit()

    logger.info(f"ğŸ¤– ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì‹œì‘: {complex_name} (Job ID: {job_id})")

    result = {
        "job_id": job_id,
        "complex_id": complex_id,
        "complex_name": complex_name,
        "started_at": job.started_at.isoformat(),
        "success": False,
        "error": None
    }

    try:
        # í¬ë¡¤ë§ ì‹¤í–‰ (ê²°ê³¼ í¬í•¨)
        crawl_result = asyncio.run(crawl_single_complex_with_result(complex_id, db))

        # ì‘ì—… ì„±ê³µ ì²˜ë¦¬
        job.status = 'success'
        job.finished_at = datetime.now(timezone.utc)
        job.duration_seconds = int((job.finished_at - job.started_at).total_seconds())
        job.articles_collected = crawl_result["articles_collected"]
        job.articles_new = crawl_result["articles_new"]
        job.articles_updated = crawl_result["articles_updated"]
        db.commit()

        result["success"] = True
        result["finished_at"] = job.finished_at.isoformat()
        result["duration_seconds"] = job.duration_seconds
        result["articles_collected"] = job.articles_collected
        result["articles_new"] = job.articles_new

        logger.info(f"âœ… ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì™„ë£Œ: {complex_name} (ìˆ˜ì§‘: {job.articles_collected}ê±´)")

    except Exception as e:
        # ì‘ì—… ì‹¤íŒ¨ ì²˜ë¦¬
        job.status = 'failed'
        job.finished_at = datetime.now(timezone.utc)
        job.duration_seconds = int((job.finished_at - job.started_at).total_seconds())
        job.error_message = str(e)
        job.error_traceback = traceback.format_exc()
        db.commit()

        result["error"] = str(e)
        logger.error(f"âŒ ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì‹¤íŒ¨: {complex_name} - {str(e)}")

    finally:
        db.close()

    return result
