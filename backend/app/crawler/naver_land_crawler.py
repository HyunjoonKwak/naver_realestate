"""
ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ëŸ¬ - ì™„ì „íŒ
ë‹¨ì§€ ì •ë³´, ë§¤ë¬¼, ì‹¤ê±°ë˜ê°€ ëª¨ë‘ ìˆ˜ì§‘
"""
import asyncio
import json
from typing import Dict, List, Optional
from playwright.async_api import async_playwright, Page


class NaverLandCrawler:
    """ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ëŸ¬"""

    def __init__(self, headless: bool = True):
        self.headless = headless
        self.browser = None
        self.context = None
        self.page = None

    async def __aenter__(self):
        await self.start()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close()

    async def start(self):
        """ë¸Œë¼ìš°ì € ì‹œì‘"""
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(
            headless=self.headless,
            args=['--disable-blink-features=AutomationControlled']
        )
        self.context = await self.browser.new_context(
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )
        self.page = await self.context.new_page()

    async def close(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        if self.browser:
            await self.browser.close()

    async def crawl_complex(self, complex_id: str) -> Dict:
        """
        ë‹¨ì§€ ì „ì²´ ì •ë³´ í¬ë¡¤ë§

        Args:
            complex_id: ë‹¨ì§€ ID

        Returns:
            {
                'complex': ë‹¨ì§€ ìƒì„¸ ì •ë³´,
                'articles': ë§¤ë¬¼ ëª©ë¡,
                'trades': ì‹¤ê±°ë˜ê°€ ëª©ë¡
            }
        """
        print(f"\n{'='*80}")
        print(f"ğŸ¢ ë‹¨ì§€ í¬ë¡¤ë§: ID {complex_id}")
        print(f"{'='*80}")

        url = f"https://new.land.naver.com/complexes/{complex_id}"

        # API ì‘ë‹µ ì €ì¥ì†Œ
        captured = {
            'complex': None,
            'articles': None,
            'trades': None,
            'all_responses': []
        }

        # ì‘ë‹µ ë¦¬ìŠ¤ë„ˆ
        async def save_response(response):
            if '/api/' in response.url and response.status == 200:
                try:
                    data = await response.json()
                    captured['all_responses'].append({
                        'url': response.url,
                        'data': data
                    })

                    # ë‹¨ì§€ ìƒì„¸ ì •ë³´
                    if f'complexes/{complex_id}?' in response.url and 'complexNo=' in response.url:
                        captured['complex'] = data
                        print(f"âœ… ë‹¨ì§€ ì •ë³´: {data.get('complexName', 'Unknown')}")

                    # ë§¤ë¬¼ ì •ë³´
                    elif f'{complex_id}?' in response.url and 'realEstateType=' in response.url:
                        captured['articles'] = data
                        article_count = len(data.get('articleList', []))
                        print(f"âœ… ë§¤ë¬¼ ì •ë³´: {article_count}ê±´")

                    # ì‹¤ê±°ë˜ê°€
                    elif 'real-trades' in response.url or 'trade' in response.url.lower():
                        captured['trades'] = data
                        print(f"âœ… ì‹¤ê±°ë˜ê°€ ì •ë³´")

                except:
                    pass

        self.page.on("response", save_response)

        # í˜ì´ì§€ ë¡œë“œ
        print(f"\nâ³ í˜ì´ì§€ ë¡œë”©...")
        await self.page.goto(url)
        await self.page.wait_for_load_state("networkidle")
        await asyncio.sleep(2)

        # ìŠ¤í¬ë¡¤
        print(f"\nğŸ“œ ìŠ¤í¬ë¡¤...")
        for _ in range(2):
            await self.page.evaluate("window.scrollBy(0, 400)")
            await asyncio.sleep(0.5)

        # íƒ­ í´ë¦­ (ì‹¤ê±°ë˜ê°€ íƒ­ ë“±)
        print(f"\nğŸ–±ï¸  íƒ­ í´ë¦­...")
        buttons = await self.page.query_selector_all("button, a, [role='tab']")

        clicked = 0
        for btn in buttons:
            if clicked >= 3:  # ìµœëŒ€ 3ê°œë§Œ
                break
            try:
                text = await btn.inner_text()
                # ì‹¤ê±°ë˜, ì‹œì„¸ ë“±ì˜ íƒ­ í´ë¦­
                if text and any(kw in text for kw in ["ì‹¤ê±°ë˜", "ê±°ë˜", "ì‹œì„¸"]):
                    print(f"   í´ë¦­: {text.strip()}")
                    await btn.click()
                    await asyncio.sleep(2)
                    clicked += 1
            except:
                pass

        # ì¶”ê°€ ëŒ€ê¸°
        await asyncio.sleep(1)

        print(f"\n{'='*80}")
        print(f"ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ")
        print(f"{'='*80}")

        return {
            'complex_id': complex_id,
            'complex_detail': captured['complex'],
            'articles': captured['articles'],
            'trades': captured['trades'],
            'raw_responses': captured['all_responses']
        }

    def parse_complex_detail(self, data: Dict) -> Dict:
        """ë‹¨ì§€ ìƒì„¸ ì •ë³´ íŒŒì‹±"""
        if not data:
            return None

        return {
            'complex_id': data.get('complexNo'),
            'name': data.get('complexName'),
            'type': data.get('complexTypeName'),
            'address': data.get('address'),
            'total_households': data.get('totalHouseHoldCount'),
            'total_dongs': data.get('totalDongCount'),
            'completion_date': data.get('useApproveYmd'),
            'min_area': data.get('minArea'),
            'max_area': data.get('maxArea'),
            'min_price': data.get('minPrice'),
            'max_price': data.get('maxPrice'),
            'min_lease_price': data.get('minLeasePrice'),
            'max_lease_price': data.get('maxLeasePrice'),
            'latitude': data.get('latitude'),
            'longitude': data.get('longitude'),
            'pyeongs': data.get('pyeongs', []),
            'dongs': data.get('dongs', [])
        }

    def parse_articles(self, data: Dict) -> List[Dict]:
        """ë§¤ë¬¼ ì •ë³´ íŒŒì‹±"""
        if not data or 'articleList' not in data:
            return []

        articles = []
        for article in data['articleList']:
            articles.append({
                'article_no': article.get('articleNo'),
                'trade_type': article.get('tradeTypeName'),
                'price': article.get('dealOrWarrantPrc'),
                'area_name': article.get('areaName'),
                'area1': article.get('area1'),
                'area2': article.get('area2'),
                'floor_info': article.get('floorInfo'),
                'direction': article.get('direction'),
                'confirm_date': article.get('articleConfirmYmd'),
                'building_name': article.get('buildingName'),
                'feature_desc': article.get('articleFeatureDesc'),
                'tags': article.get('tagList', []),
                'realtor_name': article.get('realtorName'),
                'latitude': article.get('latitude'),
                'longitude': article.get('longitude')
            })

        return articles

    def parse_trades(self, data: Dict) -> List[Dict]:
        """ì‹¤ê±°ë˜ê°€ ì •ë³´ íŒŒì‹±"""
        if not data:
            return []

        trades = []
        trade_list = data.get('list', [])

        for trade in trade_list:
            trades.append({
                'deal_date': trade.get('dealDate'),
                'deal_price': trade.get('dealAmount'),
                'area': trade.get('area'),
                'floor': trade.get('floor'),
                'trade_type': trade.get('tradeTypeName')
            })

        return trades


async def test_crawler():
    """í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""

    async with NaverLandCrawler(headless=False) as crawler:
        # ë‹¨ì§€ í¬ë¡¤ë§
        result = await crawler.crawl_complex("109208")

        # íŒŒì‹±
        complex_detail = crawler.parse_complex_detail(result['complex_detail'])
        articles = crawler.parse_articles(result['articles'])
        trades = crawler.parse_trades(result['trades'])

        # ê²°ê³¼ ì¶œë ¥
        print(f"\n{'='*80}")
        print("ğŸ“‹ íŒŒì‹± ê²°ê³¼")
        print(f"{'='*80}")

        if complex_detail:
            print(f"\nğŸ¢ ë‹¨ì§€ ì •ë³´:")
            print(f"   ì´ë¦„: {complex_detail['name']}")
            print(f"   ì£¼ì†Œ: {complex_detail.get('address', 'N/A')}")
            print(f"   ì„¸ëŒ€ìˆ˜: {complex_detail['total_households']}")
            print(f"   ì¤€ê³µ: {complex_detail['completion_date']}")
            print(f"   ê°€ê²©ëŒ€: {complex_detail['min_price']}ë§Œì› ~ {complex_detail['max_price']}ë§Œì›")

        print(f"\nğŸ’° ë§¤ë¬¼: {len(articles)}ê±´")
        for i, art in enumerate(articles[:5], 1):
            print(f"   {i}. {art['price']} / {art['area_name']} / {art['floor_info']} / {art['direction']}")

        print(f"\nğŸ“Š ì‹¤ê±°ë˜: {len(trades)}ê±´")
        for i, trade in enumerate(trades[:5], 1):
            print(f"   {i}. {trade.get('deal_date', 'N/A')} / {trade.get('deal_price', 'N/A')}ë§Œì›")

        # íŒŒì¼ ì €ì¥
        output = {
            'complex': complex_detail,
            'articles': articles,
            'trades': trades
        }

        with open('crawled_data.json', 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ ì €ì¥: crawled_data.json")
        print(f"\nâœ… ì™„ë£Œ!")


if __name__ == "__main__":
    asyncio.run(test_crawler())
