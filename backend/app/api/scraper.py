"""
ë„¤ì´ë²„ ë¶€ë™ì‚° ìŠ¤í¬ë˜í•‘ API
"""
import re
import asyncio
from datetime import datetime
from typing import Optional
from fastapi import APIRouter, HTTPException, Query, BackgroundTasks, Depends
from pydantic import BaseModel
from playwright.async_api import async_playwright
from sqlalchemy.orm import Session
from ..core.database import get_db
from ..models.complex import Complex, Article, Transaction
from ..services.article_tracker import ArticleTracker

router = APIRouter(prefix="/scraper", tags=["scraper"])


class CrawlRequest(BaseModel):
    complex_id: str


@router.get("/complex")
async def scrape_complex_info(url: str = Query(..., description="ë„¤ì´ë²„ ë¶€ë™ì‚° URL")):
    """
    ë„¤ì´ë²„ ë¶€ë™ì‚° URLì—ì„œ ë‹¨ì§€ ì •ë³´ ì¶”ì¶œ

    - **url**: ë„¤ì´ë²„ ë¶€ë™ì‚° ë‹¨ì§€ URL (ì˜ˆ: https://new.land.naver.com/complexes/1482)
    """
    # URLì—ì„œ ë‹¨ì§€ ID ì¶”ì¶œ
    match = re.search(r'/complexes/(\d+)', url)
    if not match:
        raise HTTPException(status_code=400, detail="ìœ íš¨í•˜ì§€ ì•Šì€ ë„¤ì´ë²„ ë¶€ë™ì‚° URLì…ë‹ˆë‹¤")

    complex_id = match.group(1)

    complex_data = None
    articles_data = None
    api_responses = []

    try:
        async def save_response(response):
            """API ì‘ë‹µ ì €ì¥"""
            nonlocal complex_data, articles_data
            try:
                # ëª¨ë“  API ì‘ë‹µ ê¸°ë¡
                if '/api/' in response.url:
                    api_responses.append(f"{response.status} - {response.url}")

                # ë‹¨ì§€ ì •ë³´ API ì‘ë‹µ í™•ì¸
                if '/api/complexes/overview/' in response.url and response.status == 200:
                    data = await response.json()
                    if data.get('complexNo') or data.get('complexName'):
                        complex_data = data
                        print(f"âœ… ë‹¨ì§€ ì •ë³´ ìˆ˜ì§‘: {data.get('complexName', 'N/A')}")

                # ë§¤ë¬¼ ì •ë³´ API ì‘ë‹µ í™•ì¸
                if '/api/articles/complex/' in response.url and response.status == 200:
                    data = await response.json()
                    if isinstance(data, dict) and 'articleList' in data:
                        articles_data = data
                        count = len(data.get('articleList', []))
                        print(f"âœ… ë§¤ë¬¼ ì •ë³´ ìˆ˜ì§‘: {count}ê±´")
            except Exception as e:
                print(f"âŒ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")

        # Playwrightë¡œ í˜ì´ì§€ ë°©ë¬¸í•˜ì—¬ API ì‘ë‹µ ê°€ë¡œì±„ê¸°
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                    '--no-sandbox',
                    '--disable-web-security',
                    '--disable-features=IsolateOrigins,site-per-process'
                ]
            )

            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080},
                locale='ko-KR',
                timezone_id='Asia/Seoul'
            )

            # JavaScriptë¡œ webdriver ê°ì§€ ì°¨ë‹¨
            await context.add_init_script('''
                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
                Object.defineProperty(navigator, 'languages', {get: () => ['ko-KR', 'ko']});
            ''')

            page = await context.new_page()

            # ì‘ë‹µ ë¦¬ìŠ¤ë„ˆ ë“±ë¡
            page.on("response", lambda response: asyncio.create_task(save_response(response)))

            # ë„¤ì´ë²„ í˜ì´ì§€ ë°©ë¬¸
            print(f"ğŸŒ í˜ì´ì§€ ì ‘ì† ì¤‘: {url}")
            await page.goto(url, timeout=30000)
            await asyncio.sleep(5)  # API ì‘ë‹µ ëŒ€ê¸°

            await browser.close()

        print(f"ğŸ“Š ìº¡ì²˜ëœ API ì‘ë‹µ ìˆ˜: {len(api_responses)}")
        for resp in api_responses[:5]:  # ìµœëŒ€ 5ê°œë§Œ ë¡œê·¸
            print(f"  - {resp}")

        if not complex_data:
            raise HTTPException(
                status_code=404,
                detail=f"ë‹¨ì§€ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìº¡ì²˜ëœ API: {len(api_responses)}ê°œ"
            )

        # ì‘ë‹µ ë°ì´í„° ë§¤í•‘ (ë„¤ì´ë²„ API í•„ë“œëª…ì— ë§ì¶¤)
        result = {
            'complex_id': str(complex_id),
            'complex_name': complex_data.get('complexName', ''),
            'complex_type': complex_data.get('complexTypeName', 'ì•„íŒŒíŠ¸'),
            'total_households': complex_data.get('totalHouseHoldCount'),  # ëŒ€ë¬¸ì H
            'total_dongs': complex_data.get('totalDongCount'),
            'latitude': complex_data.get('latitude'),
            'longitude': complex_data.get('longitude'),
        }

        # ì¤€ê³µì¼ í¬ë§·: YYYYMMDD -> YYYY-MM
        use_approve_ymd = complex_data.get('useApproveYmd')
        if use_approve_ymd and len(use_approve_ymd) >= 6:
            result['completion_date'] = f"{use_approve_ymd[:4]}-{use_approve_ymd[4:6]}"

        # ë©´ì  ì •ë³´ (APIì—ì„œ ì§ì ‘ ì œê³µ)
        result['min_area'] = complex_data.get('minArea')
        result['max_area'] = complex_data.get('maxArea')

        # ê°€ê²© ì •ë³´ (ë§Œì› ë‹¨ìœ„)
        result['min_price'] = complex_data.get('minPrice')
        result['max_price'] = complex_data.get('maxPrice')
        result['min_lease_price'] = complex_data.get('minLeasePrice')
        result['max_lease_price'] = complex_data.get('maxLeasePrice')

        # ë§¤ë¬¼ ì •ë³´ ì¶”ê°€
        result['articles'] = []
        if articles_data and 'articleList' in articles_data:
            result['articles'] = articles_data.get('articleList', [])
            result['article_count'] = len(result['articles'])

        return result

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")


@router.post("/crawl")
async def crawl_complex(request: CrawlRequest, background_tasks: BackgroundTasks, db: Session = Depends(get_db)):
    """
    ë‹¨ì§€ì˜ ë§¤ë¬¼ ì •ë³´ í¬ë¡¤ë§ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)

    - **complex_id**: í¬ë¡¤ë§í•  ë‹¨ì§€ ID
    """
    # ë‹¨ì§€ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    complex = db.query(Complex).filter(Complex.complex_id == request.complex_id).first()
    if not complex:
        raise HTTPException(status_code=404, detail="ë‹¨ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰ (db ì„¸ì…˜ì€ ì „ë‹¬í•˜ì§€ ì•ŠìŒ - save_to_databaseê°€ ìì²´ ì„¸ì…˜ ìƒì„±)
    background_tasks.add_task(run_crawler, request.complex_id)

    return {
        "message": "í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤",
        "complex_id": request.complex_id,
        "complex_name": complex.complex_name
    }


async def run_crawler(complex_id: str, create_snapshot: bool = False):
    """ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰ (advanced_crawler.py ë¡œì§ ì´ìš©)"""
    import sys
    import os
    import traceback
    from ..core.database import SessionLocal

    print(f"   [SCRAPER] Starting run_crawler for complex {complex_id}")

    # advanced_crawler.pyë¥¼ ì„í¬íŠ¸í•˜ê¸° ìœ„í•´ ê²½ë¡œ ì¶”ê°€
    backend_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    project_root = os.path.dirname(backend_path)
    sys.path.insert(0, project_root)

    print(f"   [SCRAPER] Project root: {project_root}")

    try:
        print(f"   [SCRAPER] Importing NaverRealEstateCrawler...")
        from advanced_crawler import NaverRealEstateCrawler

        print(f"   [SCRAPER] Creating crawler instance...")
        crawler = NaverRealEstateCrawler()

        print(f"   [SCRAPER] Starting crawl_complex...")
        await crawler.crawl_complex(complex_id)

        print(f"   [SCRAPER] Saving to database...")
        crawler.save_to_database(complex_id)

        # ìŠ¤ëƒ…ìƒ· ìƒì„± ë° ë³€ë™ì‚¬í•­ ê°ì§€
        if create_snapshot:
            print(f"   [SCRAPER] Creating snapshot and detecting changes...")
            db = SessionLocal()
            try:
                tracker = ArticleTracker(db)

                # í˜„ì¬ ë§¤ë¬¼ ì¡°íšŒ
                articles = db.query(Article).filter(
                    Article.complex_id == complex_id,
                    Article.is_active == True
                ).all()

                # ìŠ¤ëƒ…ìƒ· ìƒì„±
                tracker.create_snapshot(complex_id, articles)

                # ë³€ë™ì‚¬í•­ ê°ì§€
                tracker.detect_changes(complex_id)

            finally:
                db.close()

        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {complex_id}")
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {complex_id} - {e}")
        print(f"   [SCRAPER] Full traceback:")
        traceback.print_exc()


# í¬ë¡¤ë§ ìƒíƒœ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
crawling_status = {}

@router.post("/refresh/{complex_id}")
async def refresh_and_track(
    complex_id: str,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """
    ë§¤ë¬¼ ëª©ë¡ ìƒˆë¡œê³ ì¹¨ ë° ë³€ë™ì‚¬í•­ ì¶”ì 

    - í¬ë¡¤ë§ ì‹¤í–‰
    - ìŠ¤ëƒ…ìƒ· ìƒì„±
    - ë³€ë™ì‚¬í•­ ìë™ ê°ì§€

    Args:
        complex_id: ë‹¨ì§€ ID
    """
    # ë‹¨ì§€ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    complex = db.query(Complex).filter(Complex.complex_id == complex_id).first()
    if not complex:
        raise HTTPException(status_code=404, detail="ë‹¨ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # í¬ë¡¤ë§ ìƒíƒœ ì´ˆê¸°í™”
    crawling_status[complex_id] = {
        "status": "running",
        "started_at": datetime.now().isoformat()
    }

    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ + ìŠ¤ëƒ…ìƒ· ìƒì„± + ë³€ë™ ê°ì§€ ì‹¤í–‰
    async def crawl_with_status_update():
        try:
            await run_crawler(complex_id, True)
            crawling_status[complex_id] = {
                "status": "completed",
                "completed_at": datetime.now().isoformat()
            }
        except Exception as e:
            crawling_status[complex_id] = {
                "status": "failed",
                "error": str(e),
                "failed_at": datetime.now().isoformat()
            }

    background_tasks.add_task(crawl_with_status_update)

    return {
        "message": "ë§¤ë¬¼ ìƒˆë¡œê³ ì¹¨ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì™„ë£Œ í›„ ë³€ë™ì‚¬í•­ì´ ìë™ìœ¼ë¡œ ê°ì§€ë©ë‹ˆë‹¤.",
        "complex_id": complex_id,
        "complex_name": complex.complex_name
    }


@router.get("/refresh/{complex_id}/status")
def get_refresh_status(complex_id: str):
    """
    í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ

    Args:
        complex_id: ë‹¨ì§€ ID

    Returns:
        ìƒíƒœ ì •ë³´ (running, completed, failed)
    """
    status = crawling_status.get(complex_id, {"status": "not_found"})
    return {
        "complex_id": complex_id,
        **status
    }
