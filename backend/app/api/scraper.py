"""
ë„¤ì´ë²„ ë¶€ë™ì‚° ìŠ¤í¬ë˜í•‘ API
"""
import re
import asyncio
from datetime import datetime
from typing import Optional
from fastapi import APIRouter, HTTPException, Query, BackgroundTasks, Depends
from pydantic import BaseModel
from playwright.async_api import async_playwright
from sqlalchemy.orm import Session
from ..core.database import get_db
from ..models.complex import Complex, Article, Transaction
from ..services.article_tracker import ArticleTracker

router = APIRouter(prefix="/scraper", tags=["scraper"])


class CrawlRequest(BaseModel):
    complex_id: str
    collect_address: bool = False


@router.get("/complex")
async def scrape_complex_info(url: str = Query(..., description="ë„¤ì´ë²„ ë¶€ë™ì‚° URL")):
    """
    ë„¤ì´ë²„ ë¶€ë™ì‚° URLì—ì„œ ë‹¨ì§€ ì •ë³´ ì¶”ì¶œ

    - **url**: ë„¤ì´ë²„ ë¶€ë™ì‚° ë‹¨ì§€ URL (ì˜ˆ: https://new.land.naver.com/complexes/1482)
    """
    # URLì—ì„œ ë‹¨ì§€ ID ì¶”ì¶œ
    match = re.search(r'/complexes/(\d+)', url)
    if not match:
        raise HTTPException(status_code=400, detail="ìœ íš¨í•˜ì§€ ì•Šì€ ë„¤ì´ë²„ ë¶€ë™ì‚° URLì…ë‹ˆë‹¤")

    complex_id = match.group(1)

    complex_data = None
    articles_data = None
    api_responses = []

    try:
        async def save_response(response):
            """API ì‘ë‹µ ì €ì¥"""
            nonlocal complex_data, articles_data
            try:
                # ëª¨ë“  API ì‘ë‹µ ê¸°ë¡
                if '/api/' in response.url:
                    api_responses.append(f"{response.status} - {response.url}")

                # ë‹¨ì§€ ì •ë³´ API ì‘ë‹µ í™•ì¸
                if '/api/complexes/overview/' in response.url and response.status == 200:
                    data = await response.json()
                    if data.get('complexNo') or data.get('complexName'):
                        complex_data = data
                        print(f"âœ… ë‹¨ì§€ ì •ë³´ ìˆ˜ì§‘: {data.get('complexName', 'N/A')}")

                # ë§¤ë¬¼ ì •ë³´ API ì‘ë‹µ í™•ì¸
                if '/api/articles/complex/' in response.url and response.status == 200:
                    data = await response.json()
                    if isinstance(data, dict) and 'articleList' in data:
                        articles_data = data
                        count = len(data.get('articleList', []))
                        print(f"âœ… ë§¤ë¬¼ ì •ë³´ ìˆ˜ì§‘: {count}ê±´")
            except Exception as e:
                print(f"âŒ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")

        # Playwrightë¡œ í˜ì´ì§€ ë°©ë¬¸í•˜ì—¬ API ì‘ë‹µ ê°€ë¡œì±„ê¸°
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                    '--no-sandbox',
                    '--disable-web-security',
                    '--disable-features=IsolateOrigins,site-per-process'
                ]
            )

            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080},
                locale='ko-KR',
                timezone_id='Asia/Seoul'
            )

            # JavaScriptë¡œ webdriver ê°ì§€ ì°¨ë‹¨
            await context.add_init_script('''
                Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
                Object.defineProperty(navigator, 'languages', {get: () => ['ko-KR', 'ko']});
            ''')

            page = await context.new_page()

            # ì‘ë‹µ ë¦¬ìŠ¤ë„ˆ ë“±ë¡
            page.on("response", lambda response: asyncio.create_task(save_response(response)))

            # ë„¤ì´ë²„ í˜ì´ì§€ ë°©ë¬¸
            print(f"ğŸŒ í˜ì´ì§€ ì ‘ì† ì¤‘: {url}")
            await page.goto(url, timeout=30000)
            await asyncio.sleep(5)  # API ì‘ë‹µ ëŒ€ê¸°

            await browser.close()

        print(f"ğŸ“Š ìº¡ì²˜ëœ API ì‘ë‹µ ìˆ˜: {len(api_responses)}")
        for resp in api_responses[:5]:  # ìµœëŒ€ 5ê°œë§Œ ë¡œê·¸
            print(f"  - {resp}")

        if not complex_data:
            raise HTTPException(
                status_code=404,
                detail=f"ë‹¨ì§€ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìº¡ì²˜ëœ API: {len(api_responses)}ê°œ"
            )

        # ì‘ë‹µ ë°ì´í„° ë§¤í•‘ (ë„¤ì´ë²„ API í•„ë“œëª…ì— ë§ì¶¤)
        result = {
            'complex_id': str(complex_id),
            'complex_name': complex_data.get('complexName', ''),
            'complex_type': complex_data.get('complexTypeName', 'ì•„íŒŒíŠ¸'),
            'total_households': complex_data.get('totalHouseHoldCount'),  # ëŒ€ë¬¸ì H
            'total_dongs': complex_data.get('totalDongCount'),
            'latitude': complex_data.get('latitude'),
            'longitude': complex_data.get('longitude'),
        }

        # ì¤€ê³µì¼ í¬ë§·: YYYYMMDD -> YYYY-MM
        use_approve_ymd = complex_data.get('useApproveYmd')
        if use_approve_ymd and len(use_approve_ymd) >= 6:
            result['completion_date'] = f"{use_approve_ymd[:4]}-{use_approve_ymd[4:6]}"

        # ë©´ì  ì •ë³´ (APIì—ì„œ ì§ì ‘ ì œê³µ)
        result['min_area'] = complex_data.get('minArea')
        result['max_area'] = complex_data.get('maxArea')

        # ê°€ê²© ì •ë³´ (ë§Œì› ë‹¨ìœ„)
        result['min_price'] = complex_data.get('minPrice')
        result['max_price'] = complex_data.get('maxPrice')
        result['min_lease_price'] = complex_data.get('minLeasePrice')
        result['max_lease_price'] = complex_data.get('maxLeasePrice')

        # ë§¤ë¬¼ ì •ë³´ ì¶”ê°€
        result['articles'] = []
        if articles_data and 'articleList' in articles_data:
            result['articles'] = articles_data.get('articleList', [])
            result['article_count'] = len(result['articles'])

        return result

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")


@router.post("/crawl")
async def crawl_complex(request: CrawlRequest, background_tasks: BackgroundTasks, db: Session = Depends(get_db)):
    """
    ë‹¨ì§€ì˜ ë§¤ë¬¼ ì •ë³´ í¬ë¡¤ë§ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)

    - **complex_id**: í¬ë¡¤ë§í•  ë‹¨ì§€ ID
    - **collect_address**: ì£¼ì†Œ ìˆ˜ì§‘ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
    """
    # ë‹¨ì§€ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    complex = db.query(Complex).filter(Complex.complex_id == request.complex_id).first()
    if not complex:
        raise HTTPException(status_code=404, detail="ë‹¨ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # í¬ë¡¤ë§ ìƒíƒœ ì´ˆê¸°í™”
    crawling_status[request.complex_id] = {
        "status": "running",
        "started_at": datetime.now().isoformat()
    }

    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰ (ë³€ë™ ì¶”ì  í™œì„±í™”)
    background_tasks.add_task(run_crawler_with_status, request.complex_id, True, request.collect_address)

    return {
        "message": "í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤",
        "complex_id": request.complex_id,
        "complex_name": complex.complex_name,
        "collect_address": request.collect_address
    }


@router.post("/crawl/{complex_id}")
async def crawl_complex_by_id(complex_id: str, background_tasks: BackgroundTasks, db: Session = Depends(get_db)):
    """
    ë‹¨ì§€ì˜ ë§¤ë¬¼ ì •ë³´ í¬ë¡¤ë§ (RESTful ê²½ë¡œ íŒŒë¼ë¯¸í„° ë²„ì „)

    - **complex_id**: í¬ë¡¤ë§í•  ë‹¨ì§€ ID
    """
    # ë‹¨ì§€ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    complex = db.query(Complex).filter(Complex.complex_id == complex_id).first()
    if not complex:
        raise HTTPException(status_code=404, detail="ë‹¨ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰ (ë³€ë™ ì¶”ì  í™œì„±í™”)
    background_tasks.add_task(run_crawler, complex_id, True)

    return {
        "message": "í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤",
        "complex_id": complex_id,
        "complex_name": complex.complex_name
    }


async def run_crawler(complex_id: str, create_snapshot: bool = True, collect_address: bool = False):
    """
    ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰ (crawler_service ì‚¬ìš©)

    Args:
        complex_id: ë‹¨ì§€ ID
        create_snapshot: ìŠ¤ëƒ…ìƒ· ìƒì„± ë° ë³€ë™ ê°ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’ True)
        collect_address: ì£¼ì†Œ ìˆ˜ì§‘ ì—¬ë¶€ (ê¸°ë³¸ê°’ False)
    """
    import traceback
    from ..core.database import SessionLocal
    from ..services.crawler_service import NaverRealEstateCrawler

    print(f"   [SCRAPER] Starting run_crawler for complex {complex_id}")
    print(f"   [SCRAPER] Snapshot tracking: {'Enabled' if create_snapshot else 'Disabled'}")
    print(f"   [SCRAPER] Address collection: {'Enabled' if collect_address else 'Disabled'}")

    try:
        print(f"   [SCRAPER] Creating crawler instance...")
        crawler = NaverRealEstateCrawler()

        print(f"   [SCRAPER] Starting crawl_complex...")
        await crawler.crawl_complex(complex_id, collect_address=collect_address)

        print(f"   [SCRAPER] Saving to database...")
        crawler.save_to_database(complex_id)

        # ìŠ¤ëƒ…ìƒ· ìƒì„± ë° ë³€ë™ì‚¬í•­ ê°ì§€
        if create_snapshot:
            print(f"   [SCRAPER] Creating snapshot and detecting changes...")
            db = SessionLocal()
            try:
                tracker = ArticleTracker(db)

                # í˜„ì¬ ë§¤ë¬¼ ì¡°íšŒ
                articles = db.query(Article).filter(
                    Article.complex_id == complex_id,
                    Article.is_active == True
                ).all()

                # ìŠ¤ëƒ…ìƒ· ìƒì„±
                tracker.create_snapshot(complex_id, articles)

                # ë³€ë™ì‚¬í•­ ê°ì§€
                tracker.detect_changes(complex_id)

            finally:
                db.close()

        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {complex_id}")
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {complex_id} - {e}")
        print(f"   [SCRAPER] Full traceback:")
        traceback.print_exc()


# í¬ë¡¤ë§ ìƒíƒœ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
crawling_status = {}


async def run_crawler_with_status(complex_id: str, create_snapshot: bool = True, collect_address: bool = False):
    """
    ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ í¬í•¨í•œ í¬ë¡¤ë§ ì‹¤í–‰

    Args:
        complex_id: ë‹¨ì§€ ID
        create_snapshot: ìŠ¤ëƒ…ìƒ· ìƒì„± ë° ë³€ë™ ê°ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’ True)
        collect_address: ì£¼ì†Œ ìˆ˜ì§‘ ì—¬ë¶€ (ê¸°ë³¸ê°’ False)
    """
    try:
        await run_crawler(complex_id, create_snapshot, collect_address)
        crawling_status[complex_id] = {
            "status": "completed",
            "completed_at": datetime.now().isoformat()
        }
    except Exception as e:
        crawling_status[complex_id] = {
            "status": "failed",
            "error": str(e),
            "failed_at": datetime.now().isoformat()
        }


@router.get("/crawl/{complex_id}/status")
def get_crawl_status(complex_id: str):
    """
    í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ

    Args:
        complex_id: ë‹¨ì§€ ID

    Returns:
        ìƒíƒœ ì •ë³´ (running, completed, failed)
    """
    status = crawling_status.get(complex_id, {"status": "not_found"})
    return {
        "complex_id": complex_id,
        **status
    }


@router.post("/refresh/{complex_id}")
async def refresh_and_track(
    complex_id: str,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """
    ë§¤ë¬¼ ëª©ë¡ ìƒˆë¡œê³ ì¹¨ ë° ë³€ë™ì‚¬í•­ ì¶”ì 

    - í¬ë¡¤ë§ ì‹¤í–‰
    - ìŠ¤ëƒ…ìƒ· ìƒì„±
    - ë³€ë™ì‚¬í•­ ìë™ ê°ì§€

    Args:
        complex_id: ë‹¨ì§€ ID
    """
    # ë‹¨ì§€ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    complex = db.query(Complex).filter(Complex.complex_id == complex_id).first()
    if not complex:
        raise HTTPException(status_code=404, detail="ë‹¨ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # í¬ë¡¤ë§ ìƒíƒœ ì´ˆê¸°í™”
    crawling_status[complex_id] = {
        "status": "running",
        "started_at": datetime.now().isoformat()
    }

    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ + ìŠ¤ëƒ…ìƒ· ìƒì„± + ë³€ë™ ê°ì§€ + ì‹¤ê±°ë˜ê°€ ì¡°íšŒ ì‹¤í–‰
    async def crawl_with_status_update():
        try:
            await run_crawler(complex_id, True)

            # ì‹¤ê±°ë˜ê°€ ì¡°íšŒ ë° ì €ì¥
            from ..core.database import SessionLocal
            from ..services.transaction_service import TransactionService

            db_local = SessionLocal()
            try:
                transaction_service = TransactionService(db_local)
                result = transaction_service.fetch_and_save_transactions(complex_id, months=6)
                print(f"âœ… ì‹¤ê±°ë˜ê°€ ì €ì¥: {result.get('saved_count', 0)}ê±´")
            except Exception as tx_error:
                print(f"âš ï¸ ì‹¤ê±°ë˜ê°€ ì¡°íšŒ ì‹¤íŒ¨ (ë¬´ì‹œ): {tx_error}")
            finally:
                db_local.close()

            crawling_status[complex_id] = {
                "status": "completed",
                "completed_at": datetime.now().isoformat()
            }
        except Exception as e:
            crawling_status[complex_id] = {
                "status": "failed",
                "error": str(e),
                "failed_at": datetime.now().isoformat()
            }

    background_tasks.add_task(crawl_with_status_update)

    return {
        "message": "ë§¤ë¬¼ ìƒˆë¡œê³ ì¹¨ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì™„ë£Œ í›„ ë³€ë™ì‚¬í•­ì´ ìë™ìœ¼ë¡œ ê°ì§€ë©ë‹ˆë‹¤.",
        "complex_id": complex_id,
        "complex_name": complex.complex_name
    }


@router.get("/refresh/{complex_id}/status")
def get_refresh_status(complex_id: str):
    """
    í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ

    Args:
        complex_id: ë‹¨ì§€ ID

    Returns:
        ìƒíƒœ ì •ë³´ (running, completed, failed)
    """
    status = crawling_status.get(complex_id, {"status": "not_found"})
    return {
        "complex_id": complex_id,
        **status
    }


@router.get("/search-address")
async def search_address(complex_name: str = Query(..., description="ë‹¨ì§€ëª…")):
    """
    ë„¤ì´ë²„ ê²€ìƒ‰ìœ¼ë¡œ ë‹¨ì§€ ì£¼ì†Œ í¬ë¡¤ë§

    - **complex_name**: ê²€ìƒ‰í•  ë‹¨ì§€ëª…
    """
    try:
        from ..services.address_service import AddressService

        address_service = AddressService()
        address = await address_service._get_complex_address_async(complex_name)

        if address:
            return {
                "success": True,
                "complex_name": complex_name,
                "address": address
            }
        else:
            return {
                "success": False,
                "complex_name": complex_name,
                "address": None,
                "message": "ì£¼ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì£¼ì†Œ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
